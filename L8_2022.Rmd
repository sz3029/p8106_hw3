---
title: "Classification I"
author: "Yifei Sun"
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
header-includes:
- \usepackage{fancyhdr}
- \usepackage{lipsum}
- \pagestyle{fancy}
- \fancyhead[R]{\thepage}
- \fancypagestyle{plain}{\pagestyle{fancy}}
--- 

\newpage


  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

```{r, echo = T, message = FALSE, results='hide', warning=FALSE}
library(caret)
library(glmnet)
library(mlbench)
library(pROC)
library(pdp)
library(vip)
library(AppliedPredictiveModeling)
```

We use the Pima Indians Diabetes Database for illustration. The data contain 768 observations and 9 variables. The outcome is a binary variable `diabetes`. We start from some simple visualization of the data.

```{r}
data(PimaIndiansDiabetes2)
dat <- na.omit(PimaIndiansDiabetes2)

theme1 <- transparentTheme(trans = .4)
trellis.par.set(theme1)

featurePlot(x = dat[, 1:8], 
            y = dat$diabetes,
            scales = list(x = list(relation = "free"), 
                          y = list(relation = "free")),
            plot = "density", pch = "|", 
            auto.key = list(columns = 2))
```

The data is divided into two parts (training and test). 
```{r}
set.seed(1)
rowTrain <- createDataPartition(y = dat$diabetes,
                                p = 0.75,
                                list = FALSE)
```

# Logistic regression and its cousins

## `glm`

```{r}
contrasts(dat$diabetes)

glm.fit <- glm(diabetes ~ ., 
               data = dat, 
               subset = rowTrain, 
               family = binomial(link = "logit"))
```

We first consider the simple classifier with a cut-off of 0.5 and evaluate its performance on the test data.
```{r}
test.pred.prob <- predict(glm.fit, newdata = dat[-rowTrain,],
                           type = "response")
test.pred <- rep("neg", length(test.pred.prob))
test.pred[test.pred.prob>0.5] <- "pos"

confusionMatrix(data = as.factor(test.pred),
                reference = dat$diabetes[-rowTrain],
                positive = "pos")
```

We then plot the test ROC curve. You may also consider a smoothed ROC curve.
```{r}
roc.glm <- roc(dat$diabetes[-rowTrain], test.pred.prob)
plot(roc.glm, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.glm), col = 4, add = TRUE)
```

We can also fit a logistic regression using caret. This is to compare the cross-validation performance with other models, rather than tuning the model.

```{r}
# Using caret
ctrl <- trainControl(method = "repeatedcv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)
set.seed(1)
model.glm <- train(x = dat[rowTrain,1:8],
                   y = dat$diabetes[rowTrain],
                   method = "glm",
                   metric = "ROC",
                   trControl = ctrl)
```

## Penalized logistic regression

Penalized logistic regression can be fitted using `glmnet`. We use the `train` function to select the optimal tuning parameters.

```{r}
glmnGrid <- expand.grid(.alpha = seq(0, 1, length = 21),
                        .lambda = exp(seq(-8, -1, length = 50)))
set.seed(1)
model.glmn <- train(x = dat[rowTrain,1:8],
                    y = dat$diabetes[rowTrain],
                    method = "glmnet",
                    tuneGrid = glmnGrid,
                    metric = "ROC",
                    trControl = ctrl)

model.glmn$bestTune

myCol<- rainbow(25)
myPar <- list(superpose.symbol = list(col = myCol),
              superpose.line = list(col = myCol))

plot(model.glmn, par.settings = myPar, xTrans = function(x) log(x))
```

## GAM

```{r}
set.seed(1)
model.gam <- train(x = dat[rowTrain,1:8],
                   y = dat$diabetes[rowTrain],
                   method = "gam",
                   metric = "ROC",
                   trControl = ctrl)


model.gam$finalModel

plot(model.gam$finalModel, select = 3)
```


## MARS

```{r}
set.seed(1)
model.mars <- train(x = dat[rowTrain,1:8],
                    y = dat$diabetes[rowTrain],
                    method = "earth",
                    tuneGrid = expand.grid(degree = 1:4, 
                                           nprune = 2:20),
                    metric = "ROC",
                    trControl = ctrl)

plot(model.mars)

coef(model.mars$finalModel) 

pdp::partial(model.mars, pred.var = c("age"), grid.resolution = 200) %>% autoplot()

vip(model.mars$finalModel)
```


```{r}
res <- resamples(list(GLM = model.glm, 
                      GLMNET = model.glmn, 
                      GAM = model.gam,
                      MARS = model.mars))
summary(res)

bwplot(res, metric = "ROC")
```

Now let's look at the test data performance.
```{r, warning=FALSE}
glm.pred <- predict(model.glm, newdata = dat[-rowTrain,], type = "prob")[,2]
glmn.pred <- predict(model.glmn, newdata = dat[-rowTrain,], type = "prob")[,2]
gam.pred <- predict(model.gam, newdata = dat[-rowTrain,], type = "prob")[,2]
mars.pred <- predict(model.mars, newdata = dat[-rowTrain,], type = "prob")[,2]

roc.glm <- roc(dat$diabetes[-rowTrain], glm.pred)
roc.glmn <- roc(dat$diabetes[-rowTrain], glmn.pred)
roc.gam <- roc(dat$diabetes[-rowTrain], gam.pred)
roc.mars <- roc(dat$diabetes[-rowTrain], mars.pred)

auc <- c(roc.glm$auc[1], roc.glmn$auc[1], 
         roc.gam$auc[1], roc.mars$auc[1])

modelNames <- c("glm","glmn","gam","mars")

ggroc(list(roc.glm, roc.glmn, roc.gam, roc.mars), legacy.axes = TRUE) + 
  scale_color_discrete(labels = paste0(modelNames, " (", round(auc,3),")"),
                       name = "Models (AUC)") +
  geom_abline(intercept = 0, slope = 1, color = "grey")

## using plot.roc
# plot(roc.glm, legacy.axes = TRUE)
# plot(roc.glmn, col = 2, add = TRUE)
# plot(roc.gam, col = 3, add = TRUE)
# plot(roc.mars, col = 4, add = TRUE)
# 
# legend("bottomright", legend = paste0(modelNames, ": ", round(auc,3)),
#        col = 1:4, lwd = 2)
```